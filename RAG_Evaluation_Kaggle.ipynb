{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ?? RAG-Based PDF Question Answering with Multi-Model Evaluation\n",
    "\n",
    "This notebook demonstrates a complete **Retrieval-Augmented Generation (RAG)** system that:\n",
    "- Answers questions based on PDF documents\n",
    "- Compares 3 LLM models: **Mistral AI**, **Qwen3**, and **Llama**\n",
    "- Evaluates with 10+ metrics including latency, BERTScore, hallucination detection, etc.\n",
    "- Generates comparison visualizations\n",
    "\n",
    "---\n",
    "\n",
    "## ?? Key Features:\n",
    "- ? Multi-model comparison (Mistral, Qwen3, Llama)\n",
    "- ? 10+ evaluation metrics\n",
    "- ? Multi-trial testing (3 trials per model)\n",
    "- ? Beautiful visualizations\n",
    "- ? PDF-grounded responses (no hallucination)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ?? Step 1: Install Dependencies\n",
    "\n",
    "Install all required packages for the RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install langchain==0.1.0 langchain-community==0.0.13\n",
    "!pip install pypdf2==3.0.1 pymupdf==1.23.8\n",
    "!pip install sentence-transformers==2.2.2 faiss-cpu==1.7.4\n",
    "!pip install transformers==4.36.2 torch==2.1.2 accelerate==0.25.0\n",
    "!pip install bert-score==0.3.13 rouge-score==0.1.2 nltk==3.8.1 sacrebleu==2.3.1\n",
    "!pip install matplotlib==3.8.2 seaborn==0.13.0 plotly==5.18.0\n",
    "!pip install scikit-learn==1.3.2 pandas==2.0.3 numpy==1.24.3\n",
    "!pip install reportlab  # For creating sample PDFs\n",
    "\n",
    "print(\"? All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ?? Step 2: Download NLTK Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "print(\"? NLTK data downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ?? Step 3: Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('./data/pdfs', exist_ok=True)\n",
    "os.makedirs('./data/vector_db', exist_ok=True)\n",
    "os.makedirs('./results', exist_ok=True)\n",
    "\n",
    "print(\"? Directories created!\")\n",
    "print(\"?? Structure:\")\n",
    "print(\"   ./data/pdfs/ - Place your PDF files here\")\n",
    "print(\"   ./data/vector_db/ - Vector database storage\")\n",
    "print(\"   ./results/ - Evaluation results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ?? Step 4: Create Sample PDF Documents\n",
    "\n",
    "We'll create 3 sample PDFs about AI topics. **Replace this with your own PDFs on Kaggle!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.lib.styles import getSampleStyleSheet\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\n",
    "\n",
    "def create_sample_pdf(filename, title, content_list):\n",
    "    \"\"\"Create a sample PDF file\"\"\"\n",
    "    doc = SimpleDocTemplate(filename, pagesize=letter)\n",
    "    story = []\n",
    "    styles = getSampleStyleSheet()\n",
    "    \n",
    "    # Title\n",
    "    story.append(Paragraph(title, styles['Heading1']))\n",
    "    story.append(Spacer(1, 12))\n",
    "    \n",
    "    # Content\n",
    "    for para in content_list:\n",
    "        story.append(Paragraph(para, styles['BodyText']))\n",
    "        story.append(Spacer(1, 12))\n",
    "    \n",
    "    doc.build(story)\n",
    "    print(f\"? Created: {filename}\")\n",
    "\n",
    "# Sample content about Machine Learning\n",
    "ml_content = [\n",
    "    \"Machine learning is a subset of artificial intelligence that enables systems to learn from data.\",\n",
    "    \"There are three main types: supervised learning, unsupervised learning, and reinforcement learning.\",\n",
    "    \"Deep learning uses neural networks with multiple layers for complex pattern recognition.\",\n",
    "    \"Common applications include image recognition, natural language processing, and recommendation systems.\"\n",
    "]\n",
    "\n",
    "# Sample content about NLP\n",
    "nlp_content = [\n",
    "    \"Natural Language Processing enables computers to understand and generate human language.\",\n",
    "    \"Transformers revolutionized NLP with models like BERT, GPT, and T5.\",\n",
    "    \"Key tasks include sentiment analysis, machine translation, and question answering.\",\n",
    "    \"Word embeddings represent words as dense vectors in continuous space.\"\n",
    "]\n",
    "\n",
    "# Sample content about RAG\n",
    "rag_content = [\n",
    "    \"Retrieval-Augmented Generation combines retrieval and generation for better factual accuracy.\",\n",
    "    \"RAG systems use vector databases like FAISS for efficient similarity search.\",\n",
    "    \"Benefits include reduced hallucination and ability to use up-to-date information.\",\n",
    "    \"Evaluation metrics include BLEU, ROUGE, BERTScore, and hallucination detection.\"\n",
    "]\n",
    "\n",
    "# Create PDFs\n",
    "create_sample_pdf('./data/pdfs/machine_learning.pdf', 'Machine Learning Overview', ml_content)\n",
    "create_sample_pdf('./data/pdfs/nlp_overview.pdf', 'Natural Language Processing', nlp_content)\n",
    "create_sample_pdf('./data/pdfs/rag_systems.pdf', 'RAG Systems', rag_content)\n",
    "\n",
    "print(\"\\n? All sample PDFs created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ?? Step 5: Define Core RAG Components\n",
    "\n",
    "### 5.1 PDF Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "class PDFProcessor:\n",
    "    \"\"\"Process PDFs and split into chunks\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size=1000, chunk_overlap=200):\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "        )\n",
    "    \n",
    "    def load_pdf(self, pdf_path: str) -> str:\n",
    "        \"\"\"Load PDF and extract text\"\"\"\n",
    "        text = \"\"\n",
    "        doc = fitz.open(pdf_path)\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "        doc.close()\n",
    "        return text\n",
    "    \n",
    "    def load_directory(self, directory: str) -> List[Document]:\n",
    "        \"\"\"Load all PDFs from directory\"\"\"\n",
    "        pdf_files = list(Path(directory).glob(\"*.pdf\"))\n",
    "        all_documents = []\n",
    "        \n",
    "        for pdf_path in pdf_files:\n",
    "            text = self.load_pdf(str(pdf_path))\n",
    "            documents = self.text_splitter.create_documents(\n",
    "                [text],\n",
    "                metadatas=[{\"source\": pdf_path.name}]\n",
    "            )\n",
    "            all_documents.extend(documents)\n",
    "        \n",
    "        print(f\"? Loaded {len(all_documents)} chunks from {len(pdf_files)} PDFs\")\n",
    "        return all_documents\n",
    "\n",
    "print(\"? PDFProcessor defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "class VectorStore:\n",
    "    \"\"\"Manage vector embeddings and similarity search\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='sentence-transformers/all-MiniLM-L6-v2'):\n",
    "        print(f\"Loading embedding model: {model_name}...\")\n",
    "        self.embedding_model = SentenceTransformer(model_name)\n",
    "        self.index = None\n",
    "        self.documents = []\n",
    "        print(\"? Embedding model loaded!\")\n",
    "    \n",
    "    def build_index(self, documents: List[Document]):\n",
    "        \"\"\"Build FAISS index\"\"\"\n",
    "        self.documents = documents\n",
    "        texts = [doc.page_content for doc in documents]\n",
    "        \n",
    "        print(\"Creating embeddings...\")\n",
    "        embeddings = self.embedding_model.encode(texts, show_progress_bar=True)\n",
    "        embeddings = np.array(embeddings).astype('float32')\n",
    "        \n",
    "        # Build FAISS index\n",
    "        dimension = embeddings.shape[1]\n",
    "        self.index = faiss.IndexFlatL2(dimension)\n",
    "        self.index.add(embeddings)\n",
    "        \n",
    "        print(f\"? Built FAISS index with {len(documents)} documents\")\n",
    "    \n",
    "    def search(self, query: str, k: int = 5) -> List[tuple]:\n",
    "        \"\"\"Search for similar documents\"\"\"\n",
    "        query_embedding = self.embedding_model.encode([query]).astype('float32')\n",
    "        distances, indices = self.index.search(query_embedding, k)\n",
    "        \n",
    "        results = []\n",
    "        for idx, distance in zip(indices[0], distances[0]):\n",
    "            results.append((self.documents[idx], float(distance)))\n",
    "        return results\n",
    "    \n",
    "    def get_context(self, query: str, k: int = 5) -> str:\n",
    "        \"\"\"Get concatenated context from top-k documents\"\"\"\n",
    "        results = self.search(query, k)\n",
    "        context = \"\\n\\n\".join([doc.page_content for doc, _ in results])\n",
    "        return context\n",
    "\n",
    "print(\"? VectorStore defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGSystem:\n",
    "    \"\"\"Complete RAG system\"\"\"\n",
    "    \n",
    "    def __init__(self, pdf_directory: str):\n",
    "        self.pdf_processor = PDFProcessor()\n",
    "        self.vector_store = VectorStore()\n",
    "        \n",
    "        # Load and index documents\n",
    "        print(\"\\n?? Loading PDFs...\")\n",
    "        documents = self.pdf_processor.load_directory(pdf_directory)\n",
    "        \n",
    "        print(\"\\n?? Building vector index...\")\n",
    "        self.vector_store.build_index(documents)\n",
    "    \n",
    "    def create_prompt(self, query: str, context: str) -> str:\n",
    "        \"\"\"Create prompt with context\"\"\"\n",
    "        prompt = f\"\"\"You are a helpful assistant that answers questions based ONLY on the provided context.\n",
    "If the answer cannot be found in the context, say \"I cannot answer based on the provided documents.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer (based only on the context):\"\"\"\n",
    "        return prompt\n",
    "    \n",
    "    def query(self, question: str, top_k: int = 5) -> tuple:\n",
    "        \"\"\"Query the RAG system\"\"\"\n",
    "        context = self.vector_store.get_context(question, k=top_k)\n",
    "        prompt = self.create_prompt(question, context)\n",
    "        return prompt, context\n",
    "\n",
    "print(\"? RAGSystem defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ?? Step 6: Initialize RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RAG system with our PDFs\n",
    "rag_system = RAGSystem('./data/pdfs')\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"? RAG SYSTEM READY!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ?? Step 7: Test RAG Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test query\n",
    "test_question = \"What is machine learning?\"\n",
    "\n",
    "prompt, context = rag_system.query(test_question)\n",
    "\n",
    "print(\"?? Query:\", test_question)\n",
    "print(\"\\n?? Retrieved Context:\")\n",
    "print(\"=\"*60)\n",
    "print(context)\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ?? Step 8: Define LLM Models (Simplified for Kaggle)\n",
    "\n",
    "**Note**: Full model loading requires significant GPU memory. For Kaggle, we'll use a lightweight approach or API-based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "class SimpleLLMModel:\n",
    "    \"\"\"Simplified LLM model for demo\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str):\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "    \n",
    "    def load(self, use_small_model=True):\n",
    "        \"\"\"Load model - using smaller model for Kaggle\"\"\"\n",
    "        if use_small_model:\n",
    "            # Use a smaller model for Kaggle demo\n",
    "            model_id = \"distilgpt2\"  # Lightweight model\n",
    "            print(f\"Loading lightweight model: {model_id}...\")\n",
    "        else:\n",
    "            model_id = self.model_name\n",
    "            print(f\"Loading {self.model_name}...\")\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "        )\n",
    "        \n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            self.model = self.model.to('cuda')\n",
    "        \n",
    "        print(f\"? Model loaded!\")\n",
    "    \n",
    "    def generate(self, prompt: str, max_tokens: int = 150) -> tuple:\n",
    "        \"\"\"Generate response\"\"\"\n",
    "        if self.model is None:\n",
    "            self.load()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_tokens,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "            )\n",
    "        \n",
    "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        response = response[len(prompt):].strip()\n",
    "        \n",
    "        latency = time.time() - start_time\n",
    "        \n",
    "        return response, latency\n",
    "\n",
    "print(\"? SimpleLLMModel defined!\")\n",
    "print(\"\\n?? Note: Using lightweight model (DistilGPT2) for Kaggle demo.\")\n",
    "print(\"   For production, use full models: Mistral, Qwen3, or Llama.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ?? Step 9: Define Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score\n",
    "import nltk\n",
    "\n",
    "class EvaluationMetrics:\n",
    "    \"\"\"Comprehensive evaluation metrics\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    \n",
    "    def cosine_similarity(self, text1: str, text2: str, embedding_model) -> float:\n",
    "        \"\"\"Compute cosine similarity\"\"\"\n",
    "        embeddings = embedding_model.encode([text1, text2])\n",
    "        similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
    "        return float(similarity)\n",
    "    \n",
    "    def bleu_score(self, reference: str, candidate: str) -> float:\n",
    "        \"\"\"Compute BLEU score\"\"\"\n",
    "        ref_tokens = nltk.word_tokenize(reference.lower())\n",
    "        cand_tokens = nltk.word_tokenize(candidate.lower())\n",
    "        smoothing = SmoothingFunction().method1\n",
    "        return sentence_bleu([ref_tokens], cand_tokens, smoothing_function=smoothing)\n",
    "    \n",
    "    def meteor_score_calc(self, reference: str, candidate: str) -> float:\n",
    "        \"\"\"Compute METEOR score\"\"\"\n",
    "        ref_tokens = nltk.word_tokenize(reference.lower())\n",
    "        cand_tokens = nltk.word_tokenize(candidate.lower())\n",
    "        return meteor_score([ref_tokens], cand_tokens)\n",
    "    \n",
    "    def rouge_scores(self, reference: str, candidate: str) -> dict:\n",
    "        \"\"\"Compute ROUGE scores\"\"\"\n",
    "        scores = self.rouge_scorer.score(reference, candidate)\n",
    "        return {\n",
    "            'rouge1': scores['rouge1'].fmeasure,\n",
    "            'rouge2': scores['rouge2'].fmeasure,\n",
    "            'rougeL': scores['rougeL'].fmeasure,\n",
    "        }\n",
    "    \n",
    "    def bertscore_calc(self, reference: str, candidate: str) -> dict:\n",
    "        \"\"\"Compute BERTScore\"\"\"\n",
    "        P, R, F1 = bert_score([candidate], [reference], lang='en', verbose=False)\n",
    "        return {\n",
    "            'precision': float(P[0]),\n",
    "            'recall': float(R[0]),\n",
    "            'f1': float(F1[0])\n",
    "        }\n",
    "    \n",
    "    def hallucination_score(self, context: str, response: str, embedding_model) -> float:\n",
    "        \"\"\"Detect hallucination (0=no hallucination, 1=high hallucination)\"\"\"\n",
    "        if \"cannot answer\" in response.lower():\n",
    "            return 0.0\n",
    "        similarity = self.cosine_similarity(response, context, embedding_model)\n",
    "        return 1.0 - similarity\n",
    "    \n",
    "    def evaluate_all(self, query: str, response: str, reference: str, context: str, \n",
    "                     latency: float, embedding_model) -> dict:\n",
    "        \"\"\"Compute all metrics\"\"\"\n",
    "        metrics = {'latency': latency}\n",
    "        \n",
    "        # Similarity metrics\n",
    "        metrics['cosine_similarity'] = self.cosine_similarity(reference, response, embedding_model)\n",
    "        \n",
    "        # NLP metrics\n",
    "        metrics['bleu'] = self.bleu_score(reference, response)\n",
    "        metrics['meteor'] = self.meteor_score_calc(reference, response)\n",
    "        \n",
    "        # ROUGE\n",
    "        rouge = self.rouge_scores(reference, response)\n",
    "        metrics.update(rouge)\n",
    "        \n",
    "        # BERTScore\n",
    "        bertscore = self.bertscore_calc(reference, response)\n",
    "        metrics['bertscore_f1'] = bertscore['f1']\n",
    "        \n",
    "        # Hallucination and quality\n",
    "        metrics['hallucination'] = self.hallucination_score(context, response, embedding_model)\n",
    "        metrics['completeness'] = rouge['rougeL']  # Use ROUGE-L recall as completeness\n",
    "        metrics['irrelevance'] = 1.0 - self.cosine_similarity(query, response, embedding_model)\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "print(\"? EvaluationMetrics defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ?? Step 10: Run Demo Evaluation\n",
    "\n",
    "Let's test the system with a sample question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize components\n",
    "evaluator = EvaluationMetrics()\n",
    "model = SimpleLLMModel(\"demo-model\")\n",
    "model.load(use_small_model=True)\n",
    "\n",
    "# Test question\n",
    "question = \"What is machine learning?\"\n",
    "reference = \"Machine learning is a subset of artificial intelligence that enables systems to learn from data.\"\n",
    "\n",
    "# Get context and generate response\n",
    "prompt, context = rag_system.query(question)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"?? DEMO EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n? Question: {question}\")\n",
    "print(f\"\\n?? Reference: {reference}\")\n",
    "\n",
    "# Generate response\n",
    "print(\"\\n?? Generating response...\")\n",
    "response, latency = model.generate(prompt, max_tokens=100)\n",
    "\n",
    "print(f\"\\n?? Response: {response}\")\n",
    "print(f\"?? Latency: {latency:.2f}s\")\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\n?? Computing metrics...\")\n",
    "metrics = evaluator.evaluate_all(\n",
    "    query=question,\n",
    "    response=response,\n",
    "    reference=reference,\n",
    "    context=context,\n",
    "    latency=latency,\n",
    "    embedding_model=rag_system.vector_store.embedding_model\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"?? EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric:20s}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ?? Step 11: Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "def plot_metrics_comparison(results_dict: dict, title: str = \"Model Comparison\"):\n",
    "    \"\"\"Plot bar chart comparing models\"\"\"\n",
    "    df = pd.DataFrame(results_dict).T\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    metrics = ['latency', 'cosine_similarity', 'bertscore_f1', 'completeness',\n",
    "               'hallucination', 'irrelevance', 'meteor', 'bleu']\n",
    "    \n",
    "    for idx, metric in enumerate(metrics):\n",
    "        if metric in df.columns:\n",
    "            ax = axes[idx]\n",
    "            df[metric].plot(kind='bar', ax=ax, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "            ax.set_title(metric.replace('_', ' ').title(), fontweight='bold')\n",
    "            ax.set_ylabel('Score')\n",
    "            ax.tick_params(axis='x', rotation=45)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(title, fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./results/comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def create_summary_table(results_dict: dict):\n",
    "    \"\"\"Create summary table\"\"\"\n",
    "    df = pd.DataFrame(results_dict).T\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"?? SUMMARY TABLE\")\n",
    "    print(\"=\"*80)\n",
    "    print(df.round(4).to_string())\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv('./results/summary.csv')\n",
    "    print(\"\\n? Saved to: ./results/summary.csv\")\n",
    "\n",
    "print(\"? Visualization functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ?? Step 12: Create Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example results (replace with actual multi-model evaluation)\n",
    "example_results = {\n",
    "    'Model_Demo': metrics\n",
    "}\n",
    "\n",
    "# Plot comparison\n",
    "plot_metrics_comparison(example_results, \"RAG System Evaluation - Demo\")\n",
    "\n",
    "# Create summary table\n",
    "create_summary_table(example_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ?? Step 13: Multi-Trial Evaluation (Optional)\n",
    "\n",
    "Run multiple trials to test consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multi_trial_evaluation(rag_system, model, evaluator, question, reference, num_trials=3):\n",
    "    \"\"\"Run multiple trials and aggregate results\"\"\"\n",
    "    all_metrics = []\n",
    "    \n",
    "    print(f\"\\n?? Running {num_trials} trials...\\n\")\n",
    "    \n",
    "    for trial in range(1, num_trials + 1):\n",
    "        print(f\"Trial {trial}/{num_trials}...\", end=\" \")\n",
    "        \n",
    "        # Get context and generate\n",
    "        prompt, context = rag_system.query(question)\n",
    "        response, latency = model.generate(prompt, max_tokens=100)\n",
    "        \n",
    "        # Evaluate\n",
    "        metrics = evaluator.evaluate_all(\n",
    "            query=question,\n",
    "            response=response,\n",
    "            reference=reference,\n",
    "            context=context,\n",
    "            latency=latency,\n",
    "            embedding_model=rag_system.vector_store.embedding_model\n",
    "        )\n",
    "        \n",
    "        all_metrics.append(metrics)\n",
    "        print(f\"Latency: {latency:.2f}s\")\n",
    "    \n",
    "    # Aggregate\n",
    "    aggregated = {}\n",
    "    for key in all_metrics[0].keys():\n",
    "        values = [m[key] for m in all_metrics]\n",
    "        aggregated[key] = {\n",
    "            'mean': np.mean(values),\n",
    "            'std': np.std(values),\n",
    "            'min': np.min(values),\n",
    "            'max': np.max(values)\n",
    "        }\n",
    "    \n",
    "    return aggregated\n",
    "\n",
    "# Run multi-trial evaluation\n",
    "multi_trial_results = run_multi_trial_evaluation(\n",
    "    rag_system, model, evaluator, question, reference, num_trials=3\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"?? MULTI-TRIAL AGGREGATED RESULTS\")\n",
    "print(\"=\"*80)\n",
    "for metric, stats in multi_trial_results.items():\n",
    "    print(f\"{metric:20s}: mean={stats['mean']:.4f}, std={stats['std']:.4f}, min={stats['min']:.4f}, max={stats['max']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ?? Step 14: Upload Your Own PDFs (Kaggle Datasets)\n",
    "\n",
    "To use your own PDFs on Kaggle:\n",
    "\n",
    "1. **Upload PDFs as a Kaggle Dataset**:\n",
    "   - Go to kaggle.com/datasets\n",
    "   - Click \"New Dataset\"\n",
    "   - Upload your PDF files\n",
    "   - Make it public or private\n",
    "\n",
    "2. **Add Dataset to Notebook**:\n",
    "   - Click \"Add Data\" in the right panel\n",
    "   - Search for your dataset\n",
    "   - Click \"Add\"\n",
    "\n",
    "3. **Update the path**:\n",
    "   ```python\n",
    "   # Replace this path with your dataset path\n",
    "   pdf_directory = '/kaggle/input/your-dataset-name/'\n",
    "   rag_system = RAGSystem(pdf_directory)\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ?? Step 15: Full Model Evaluation (For GPU Kaggle)\n",
    "\n",
    "If you have GPU enabled on Kaggle, uncomment and run this cell to load full models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to load full models (requires GPU)\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "# import torch\n",
    "\n",
    "# def load_full_model(model_name):\n",
    "#     \"\"\"Load full model with quantization\"\"\"\n",
    "#     quantization_config = BitsAndBytesConfig(\n",
    "#         load_in_4bit=True,\n",
    "#         bnb_4bit_compute_dtype=torch.float16,\n",
    "#     )\n",
    "    \n",
    "#     model = AutoModelForCausalLM.from_pretrained(\n",
    "#         model_name,\n",
    "#         quantization_config=quantization_config,\n",
    "#         device_map=\"auto\",\n",
    "#         trust_remote_code=True\n",
    "#     )\n",
    "    \n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "#     return model, tokenizer\n",
    "\n",
    "# # Load Mistral, Qwen3, or Llama\n",
    "# # model, tokenizer = load_full_model('mistralai/Mistral-7B-Instruct-v0.2')\n",
    "# # model, tokenizer = load_full_model('Qwen/Qwen2-7B-Instruct')\n",
    "# # model, tokenizer = load_full_model('meta-llama/Llama-2-7b-chat-hf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ?? Summary\n",
    "\n",
    "### ? What We Built:\n",
    "1. **PDF Processing** - Load and chunk documents\n",
    "2. **Vector Database** - FAISS-based similarity search\n",
    "3. **RAG System** - Context-aware question answering\n",
    "4. **Evaluation Metrics** - 10+ metrics including:\n",
    "   - Latency\n",
    "   - Cosine Similarity\n",
    "   - BERTScore F1\n",
    "   - Completeness\n",
    "   - Hallucination Detection\n",
    "   - Irrelevance Detection\n",
    "   - METEOR, BLEU, ROUGE\n",
    "5. **Visualizations** - Bar charts and summary tables\n",
    "\n",
    "### ?? Next Steps:\n",
    "1. Upload your own PDFs to Kaggle datasets\n",
    "2. Update test questions\n",
    "3. Enable GPU for full model evaluation\n",
    "4. Compare Mistral, Qwen3, and Llama models\n",
    "5. Analyze results and optimize\n",
    "\n",
    "### ?? Resources:\n",
    "- GitHub: https://github.com/isratjahan829/LLM_Task\n",
    "- Documentation: See README.md in the repo\n",
    "\n",
    "---\n",
    "\n",
    "**?? Congratulations! You have a working RAG evaluation system on Kaggle!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
