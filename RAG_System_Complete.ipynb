{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ?? Complete RAG System with Multi-Model Evaluation\n",
    "\n",
    "## Pure Python Implementation - No Shell Commands\n",
    "\n",
    "### Features:\n",
    "- ? PDF-based Question Answering\n",
    "- ? 3 LLM Models: Mistral, Qwen3, Llama\n",
    "- ? 10+ Evaluation Metrics\n",
    "- ? Multi-Trial Testing\n",
    "- ? Comparison Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ?? Step 1: Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Install packages\n",
    "packages = [\n",
    "    'langchain==0.1.0',\n",
    "    'langchain-community==0.0.13',\n",
    "    'pypdf2==3.0.1',\n",
    "    'pymupdf==1.23.8',\n",
    "    'sentence-transformers==2.2.2',\n",
    "    'faiss-cpu==1.7.4',\n",
    "    'transformers==4.36.2',\n",
    "    'torch==2.1.2',\n",
    "    'accelerate==0.25.0',\n",
    "    'bert-score==0.3.13',\n",
    "    'rouge-score==0.1.2',\n",
    "    'nltk==3.8.1',\n",
    "    'matplotlib==3.8.2',\n",
    "    'seaborn==0.13.0',\n",
    "    'scikit-learn==1.3.2',\n",
    "    'pandas==2.0.3',\n",
    "    'numpy==1.24.3',\n",
    "    'reportlab'\n",
    "]\n",
    "\n",
    "for package in packages:\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', package])\n",
    "\n",
    "print(\"? All packages installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK data\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "print(\"? NLTK data downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ?? Step 2: Setup Directories and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create directories\n",
    "Path('./data/pdfs').mkdir(parents=True, exist_ok=True)\n",
    "Path('./data/vector_db').mkdir(parents=True, exist_ok=True)\n",
    "Path('./results').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"? Directories created!\")\n",
    "print(\"?? ./data/pdfs/ - PDF files\")\n",
    "print(\"?? ./data/vector_db/ - Vector database\")\n",
    "print(\"?? ./results/ - Results and charts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ?? Step 3: Create Sample PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\n",
    "\n",
    "def create_pdf(filename: str, title: str, paragraphs: List[str]):\n",
    "    \"\"\"Create a PDF document\"\"\"\n",
    "    doc = SimpleDocTemplate(filename, pagesize=letter)\n",
    "    story = []\n",
    "    styles = getSampleStyleSheet()\n",
    "    \n",
    "    # Title\n",
    "    title_style = ParagraphStyle(\n",
    "        'Title',\n",
    "        parent=styles['Heading1'],\n",
    "        fontSize=24,\n",
    "        spaceAfter=30\n",
    "    )\n",
    "    story.append(Paragraph(title, title_style))\n",
    "    story.append(Spacer(1, 12))\n",
    "    \n",
    "    # Content\n",
    "    for para in paragraphs:\n",
    "        story.append(Paragraph(para, styles['BodyText']))\n",
    "        story.append(Spacer(1, 12))\n",
    "    \n",
    "    doc.build(story)\n",
    "    return filename\n",
    "\n",
    "# Create sample PDFs\n",
    "pdfs = [\n",
    "    {\n",
    "        'filename': './data/pdfs/machine_learning.pdf',\n",
    "        'title': 'Machine Learning Overview',\n",
    "        'content': [\n",
    "            \"Machine learning is a subset of artificial intelligence that enables computer systems to learn and improve from experience without being explicitly programmed. It focuses on developing algorithms that can access data and use it to learn for themselves.\",\n",
    "            \"There are three main types of machine learning: supervised learning, where models are trained on labeled data; unsupervised learning, where models find patterns in unlabeled data; and reinforcement learning, where agents learn to make decisions by interacting with an environment.\",\n",
    "            \"Deep learning is a specialized subset of machine learning that uses artificial neural networks with multiple layers. These networks can automatically learn hierarchical representations of data, making them particularly effective for complex tasks like image recognition and natural language processing.\",\n",
    "            \"Common applications of machine learning include recommendation systems, fraud detection, image and speech recognition, predictive analytics, and autonomous vehicles. The field continues to evolve rapidly with new algorithms and techniques being developed regularly.\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'filename': './data/pdfs/natural_language_processing.pdf',\n",
    "        'title': 'Natural Language Processing',\n",
    "        'content': [\n",
    "            \"Natural Language Processing (NLP) is a branch of artificial intelligence that focuses on enabling computers to understand, interpret, and generate human language in a meaningful and useful way. It combines computational linguistics with machine learning and deep learning models.\",\n",
    "            \"Key NLP tasks include sentiment analysis, machine translation, named entity recognition, question answering, text summarization, and language generation. These tasks require understanding both the syntax and semantics of language.\",\n",
    "            \"Transformer architecture, introduced in 2017, revolutionized NLP by using self-attention mechanisms. Models like BERT, GPT, and T5 have achieved remarkable results across various NLP benchmarks and real-world applications.\",\n",
    "            \"Modern NLP systems use word embeddings and contextualized representations to capture semantic meaning. Pre-trained language models can be fine-tuned for specific tasks with relatively small amounts of task-specific data.\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'filename': './data/pdfs/rag_systems.pdf',\n",
    "        'title': 'Retrieval-Augmented Generation Systems',\n",
    "        'content': [\n",
    "            \"Retrieval-Augmented Generation (RAG) is an advanced architecture that combines information retrieval with text generation. It retrieves relevant documents from a knowledge base and uses them to generate more accurate and factual responses.\",\n",
    "            \"RAG systems consist of three main components: a retriever that finds relevant documents, a vector database for efficient similarity search, and a generator that produces responses based on retrieved context. This architecture reduces hallucination and improves factual accuracy.\",\n",
    "            \"Vector databases like FAISS, Pinecone, or Weaviate enable fast similarity search over large document collections. Documents are converted into dense vector representations using embedding models like sentence-transformers.\",\n",
    "            \"Evaluation of RAG systems requires multiple metrics including semantic similarity measures like BERTScore, factuality metrics like hallucination detection, and traditional NLP metrics like BLEU and ROUGE. Multi-dimensional evaluation ensures comprehensive assessment of system performance.\"\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "for pdf_data in pdfs:\n",
    "    create_pdf(pdf_data['filename'], pdf_data['title'], pdf_data['content'])\n",
    "    print(f\"? Created: {pdf_data['filename']}\")\n",
    "\n",
    "print(\"\\n? All PDFs created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ?? Step 4: PDF Processor Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "class PDFProcessor:\n",
    "    \"\"\"Process PDFs and chunk documents\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "        )\n",
    "    \n",
    "    def extract_text_from_pdf(self, pdf_path: str) -> str:\n",
    "        \"\"\"Extract text from PDF using PyMuPDF\"\"\"\n",
    "        text = \"\"\n",
    "        doc = fitz.open(pdf_path)\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "        doc.close()\n",
    "        return text\n",
    "    \n",
    "    def load_pdf(self, pdf_path: str) -> List[Document]:\n",
    "        \"\"\"Load single PDF and create chunks\"\"\"\n",
    "        text = self.extract_text_from_pdf(pdf_path)\n",
    "        filename = os.path.basename(pdf_path)\n",
    "        \n",
    "        documents = self.text_splitter.create_documents(\n",
    "            [text],\n",
    "            metadatas=[{\"source\": filename, \"path\": pdf_path}]\n",
    "        )\n",
    "        return documents\n",
    "    \n",
    "    def load_directory(self, directory: str) -> List[Document]:\n",
    "        \"\"\"Load all PDFs from directory\"\"\"\n",
    "        pdf_files = list(Path(directory).glob(\"*.pdf\"))\n",
    "        all_documents = []\n",
    "        \n",
    "        print(f\"Loading {len(pdf_files)} PDF files...\")\n",
    "        for pdf_path in pdf_files:\n",
    "            documents = self.load_pdf(str(pdf_path))\n",
    "            all_documents.extend(documents)\n",
    "            print(f\"  ? {pdf_path.name}: {len(documents)} chunks\")\n",
    "        \n",
    "        print(f\"\\n? Total: {len(all_documents)} chunks from {len(pdf_files)} PDFs\")\n",
    "        return all_documents\n",
    "\n",
    "print(\"? PDFProcessor class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ??? Step 5: Vector Store Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "class VectorStore:\n",
    "    \"\"\"Vector database using FAISS\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = 'sentence-transformers/all-MiniLM-L6-v2'):\n",
    "        print(f\"Loading embedding model: {model_name}\")\n",
    "        self.embedding_model = SentenceTransformer(model_name)\n",
    "        self.index = None\n",
    "        self.documents = []\n",
    "        self.embeddings = None\n",
    "        print(\"? Embedding model loaded!\")\n",
    "    \n",
    "    def create_embeddings(self, documents: List[Document]) -> np.ndarray:\n",
    "        \"\"\"Create embeddings for documents\"\"\"\n",
    "        texts = [doc.page_content for doc in documents]\n",
    "        print(f\"Creating embeddings for {len(texts)} documents...\")\n",
    "        embeddings = self.embedding_model.encode(texts, show_progress_bar=True)\n",
    "        return np.array(embeddings).astype('float32')\n",
    "    \n",
    "    def build_index(self, documents: List[Document]):\n",
    "        \"\"\"Build FAISS index\"\"\"\n",
    "        self.documents = documents\n",
    "        self.embeddings = self.create_embeddings(documents)\n",
    "        \n",
    "        # Create FAISS index\n",
    "        dimension = self.embeddings.shape[1]\n",
    "        self.index = faiss.IndexFlatL2(dimension)\n",
    "        self.index.add(self.embeddings)\n",
    "        \n",
    "        print(f\"? FAISS index built with {len(documents)} documents\")\n",
    "    \n",
    "    def search(self, query: str, k: int = 5) -> List[Tuple[Document, float]]:\n",
    "        \"\"\"Search for similar documents\"\"\"\n",
    "        query_embedding = self.embedding_model.encode([query]).astype('float32')\n",
    "        distances, indices = self.index.search(query_embedding, k)\n",
    "        \n",
    "        results = []\n",
    "        for idx, distance in zip(indices[0], distances[0]):\n",
    "            if idx < len(self.documents):\n",
    "                results.append((self.documents[idx], float(distance)))\n",
    "        return results\n",
    "    \n",
    "    def get_context(self, query: str, k: int = 5) -> str:\n",
    "        \"\"\"Get concatenated context\"\"\"\n",
    "        results = self.search(query, k)\n",
    "        context = \"\\n\\n\".join([doc.page_content for doc, _ in results])\n",
    "        return context\n",
    "    \n",
    "    def compute_similarity(self, text1: str, text2: str) -> float:\n",
    "        \"\"\"Compute cosine similarity\"\"\"\n",
    "        embeddings = self.embedding_model.encode([text1, text2])\n",
    "        norm1 = np.linalg.norm(embeddings[0])\n",
    "        norm2 = np.linalg.norm(embeddings[1])\n",
    "        \n",
    "        if norm1 == 0 or norm2 == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        similarity = np.dot(embeddings[0], embeddings[1]) / (norm1 * norm2)\n",
    "        return float(similarity)\n",
    "\n",
    "print(\"? VectorStore class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ?? Step 6: RAG System Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGSystem:\n",
    "    \"\"\"Complete RAG System\"\"\"\n",
    "    \n",
    "    def __init__(self, pdf_directory: str):\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"INITIALIZING RAG SYSTEM\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Initialize components\n",
    "        self.pdf_processor = PDFProcessor(chunk_size=1000, chunk_overlap=200)\n",
    "        self.vector_store = VectorStore()\n",
    "        \n",
    "        # Load and index documents\n",
    "        print(\"\\n?? Loading PDFs...\")\n",
    "        documents = self.pdf_processor.load_directory(pdf_directory)\n",
    "        \n",
    "        print(\"\\n?? Building vector index...\")\n",
    "        self.vector_store.build_index(documents)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"? RAG SYSTEM READY!\")\n",
    "        print(\"=\"*60)\n",
    "    \n",
    "    def create_prompt(self, query: str, context: str) -> str:\n",
    "        \"\"\"Create prompt with context\"\"\"\n",
    "        prompt = f\"\"\"You are a helpful assistant that answers questions based ONLY on the provided context from PDF documents.\n",
    "If the answer cannot be found in the context, say \"I cannot answer this question based on the provided documents.\"\n",
    "\n",
    "Context from PDFs:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer (based only on the context above):\"\"\"\n",
    "        return prompt\n",
    "    \n",
    "    def query(self, question: str, top_k: int = 5) -> Tuple[str, str]:\n",
    "        \"\"\"Query the system\"\"\"\n",
    "        context = self.vector_store.get_context(question, k=top_k)\n",
    "        prompt = self.create_prompt(question, context)\n",
    "        return prompt, context\n",
    "    \n",
    "    def get_retrieved_docs(self, question: str, top_k: int = 5) -> List[Tuple[Document, float]]:\n",
    "        \"\"\"Get retrieved documents with scores\"\"\"\n",
    "        return self.vector_store.search(question, k=top_k)\n",
    "\n",
    "print(\"? RAGSystem class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ?? Step 7: Initialize RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RAG system\n",
    "rag_system = RAGSystem('./data/pdfs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ?? Step 8: Test Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test query\n",
    "test_question = \"What is machine learning and what are its main types?\"\n",
    "\n",
    "print(\"?? Test Query:\", test_question)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Get prompt and context\n",
    "prompt, context = rag_system.query(test_question)\n",
    "\n",
    "print(\"?? Retrieved Context:\")\n",
    "print(\"=\"*60)\n",
    "print(context)\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n? Context length: {len(context)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ?? Step 9: LLM Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "class LLMModel:\n",
    "    \"\"\"LLM Model with quantization support\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str, display_name: str = None):\n",
    "        self.model_name = model_name\n",
    "        self.display_name = display_name or model_name\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    def load(self, use_quantization: bool = True):\n",
    "        \"\"\"Load model with optional quantization\"\"\"\n",
    "        print(f\"\\nLoading {self.display_name}...\")\n",
    "        print(f\"Device: {self.device}\")\n",
    "        \n",
    "        try:\n",
    "            if use_quantization and torch.cuda.is_available():\n",
    "                # 4-bit quantization for GPU\n",
    "                quantization_config = BitsAndBytesConfig(\n",
    "                    load_in_4bit=True,\n",
    "                    bnb_4bit_compute_dtype=torch.float16,\n",
    "                    bnb_4bit_use_double_quant=True,\n",
    "                    bnb_4bit_quant_type=\"nf4\"\n",
    "                )\n",
    "                \n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    self.model_name,\n",
    "                    quantization_config=quantization_config,\n",
    "                    device_map=\"auto\",\n",
    "                    trust_remote_code=True\n",
    "                )\n",
    "            else:\n",
    "                # Standard loading\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    self.model_name,\n",
    "                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "                    device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "                    trust_remote_code=True,\n",
    "                    low_cpu_mem_usage=True\n",
    "                )\n",
    "                \n",
    "                if self.device == 'cuda':\n",
    "                    self.model = self.model.to(self.device)\n",
    "            \n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                self.model_name,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            \n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "            print(f\"? {self.display_name} loaded successfully!\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"? Error loading {self.display_name}: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def generate(self, prompt: str, max_tokens: int = 512, temperature: float = 0.7) -> Tuple[str, float]:\n",
    "        \"\"\"Generate response and measure latency\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not loaded. Call load() first.\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = self.tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=2048\n",
    "        )\n",
    "        \n",
    "        if self.device == 'cuda':\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_tokens,\n",
    "                temperature=temperature,\n",
    "                do_sample=True,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "        # Decode\n",
    "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        response = response[len(prompt):].strip()\n",
    "        \n",
    "        latency = time.time() - start_time\n",
    "        return response, latency\n",
    "    \n",
    "    def unload(self):\n",
    "        \"\"\"Unload model to free memory\"\"\"\n",
    "        if self.model is not None:\n",
    "            del self.model\n",
    "            del self.tokenizer\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            print(f\"? {self.display_name} unloaded\")\n",
    "\n",
    "print(\"? LLMModel class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ?? Step 10: Evaluation Metrics Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score\n",
    "\n",
    "class EvaluationMetrics:\n",
    "    \"\"\"Comprehensive evaluation metrics\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.rouge_scorer = rouge_scorer.RougeScorer(\n",
    "            ['rouge1', 'rouge2', 'rougeL'],\n",
    "            use_stemmer=True\n",
    "        )\n",
    "    \n",
    "    def cosine_similarity(self, text1: str, text2: str, embedding_model) -> float:\n",
    "        \"\"\"Compute cosine similarity\"\"\"\n",
    "        embeddings = embedding_model.encode([text1, text2])\n",
    "        similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
    "        return float(similarity)\n",
    "    \n",
    "    def bleu_score(self, reference: str, candidate: str) -> float:\n",
    "        \"\"\"Compute BLEU score\"\"\"\n",
    "        import nltk\n",
    "        ref_tokens = nltk.word_tokenize(reference.lower())\n",
    "        cand_tokens = nltk.word_tokenize(candidate.lower())\n",
    "        smoothing = SmoothingFunction().method1\n",
    "        return float(sentence_bleu([ref_tokens], cand_tokens, smoothing_function=smoothing))\n",
    "    \n",
    "    def meteor_score(self, reference: str, candidate: str) -> float:\n",
    "        \"\"\"Compute METEOR score\"\"\"\n",
    "        import nltk\n",
    "        ref_tokens = nltk.word_tokenize(reference.lower())\n",
    "        cand_tokens = nltk.word_tokenize(candidate.lower())\n",
    "        return float(meteor_score([ref_tokens], cand_tokens))\n",
    "    \n",
    "    def rouge_scores(self, reference: str, candidate: str) -> Dict[str, float]:\n",
    "        \"\"\"Compute ROUGE scores\"\"\"\n",
    "        scores = self.rouge_scorer.score(reference, candidate)\n",
    "        return {\n",
    "            'rouge1': float(scores['rouge1'].fmeasure),\n",
    "            'rouge2': float(scores['rouge2'].fmeasure),\n",
    "            'rougeL': float(scores['rougeL'].fmeasure),\n",
    "        }\n",
    "    \n",
    "    def bertscore(self, reference: str, candidate: str) -> Dict[str, float]:\n",
    "        \"\"\"Compute BERTScore\"\"\"\n",
    "        P, R, F1 = bert_score([candidate], [reference], lang='en', verbose=False)\n",
    "        return {\n",
    "            'precision': float(P[0]),\n",
    "            'recall': float(R[0]),\n",
    "            'f1': float(F1[0])\n",
    "        }\n",
    "    \n",
    "    def completeness(self, reference: str, candidate: str) -> float:\n",
    "        \"\"\"Measure completeness (coverage of reference)\"\"\"\n",
    "        scores = self.rouge_scorer.score(reference, candidate)\n",
    "        return float(scores['rougeL'].recall)\n",
    "    \n",
    "    def hallucination_score(self, context: str, response: str, embedding_model) -> float:\n",
    "        \"\"\"Detect hallucination (0=no hallucination, 1=high hallucination)\"\"\"\n",
    "        if \"cannot answer\" in response.lower() or \"don't know\" in response.lower():\n",
    "            return 0.0\n",
    "        similarity = self.cosine_similarity(response, context, embedding_model)\n",
    "        return float(1.0 - similarity)\n",
    "    \n",
    "    def irrelevance_score(self, query: str, response: str, embedding_model) -> float:\n",
    "        \"\"\"Measure irrelevance (0=relevant, 1=irrelevant)\"\"\"\n",
    "        similarity = self.cosine_similarity(query, response, embedding_model)\n",
    "        return float(1.0 - similarity)\n",
    "    \n",
    "    def evaluate_all(self, query: str, response: str, reference: str, \n",
    "                     context: str, latency: float, embedding_model) -> Dict[str, float]:\n",
    "        \"\"\"Compute all metrics\"\"\"\n",
    "        metrics = {'latency': latency}\n",
    "        \n",
    "        # Similarity\n",
    "        metrics['cosine_similarity'] = self.cosine_similarity(reference, response, embedding_model)\n",
    "        \n",
    "        # NLP metrics\n",
    "        metrics['bleu'] = self.bleu_score(reference, response)\n",
    "        metrics['meteor'] = self.meteor_score(reference, response)\n",
    "        \n",
    "        # ROUGE\n",
    "        rouge = self.rouge_scores(reference, response)\n",
    "        metrics.update(rouge)\n",
    "        \n",
    "        # BERTScore\n",
    "        bertscore = self.bertscore(reference, response)\n",
    "        metrics['bertscore_f1'] = bertscore['f1']\n",
    "        metrics['bertscore_precision'] = bertscore['precision']\n",
    "        metrics['bertscore_recall'] = bertscore['recall']\n",
    "        \n",
    "        # Quality metrics\n",
    "        metrics['completeness'] = self.completeness(reference, response)\n",
    "        metrics['hallucination'] = self.hallucination_score(context, response, embedding_model)\n",
    "        metrics['irrelevance'] = self.irrelevance_score(query, response, embedding_model)\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "print(\"? EvaluationMetrics class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ?? Step 11: Multi-Trial Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrialEvaluator:\n",
    "    \"\"\"Run multiple trials and aggregate results\"\"\"\n",
    "    \n",
    "    def __init__(self, num_trials: int = 3):\n",
    "        self.num_trials = num_trials\n",
    "        self.evaluator = EvaluationMetrics()\n",
    "    \n",
    "    def run_trials(self, model: LLMModel, rag_system: RAGSystem, \n",
    "                   question: str, reference: str) -> Dict:\n",
    "        \"\"\"Run multiple trials for a model\"\"\"\n",
    "        print(f\"\\nRunning {self.num_trials} trials for {model.display_name}...\")\n",
    "        \n",
    "        all_trials = []\n",
    "        \n",
    "        for trial in range(1, self.num_trials + 1):\n",
    "            print(f\"  Trial {trial}/{self.num_trials}...\", end=\" \")\n",
    "            \n",
    "            # Get context and generate\n",
    "            prompt, context = rag_system.query(question)\n",
    "            response, latency = model.generate(prompt)\n",
    "            \n",
    "            # Evaluate\n",
    "            metrics = self.evaluator.evaluate_all(\n",
    "                query=question,\n",
    "                response=response,\n",
    "                reference=reference,\n",
    "                context=context,\n",
    "                latency=latency,\n",
    "                embedding_model=rag_system.vector_store.embedding_model\n",
    "            )\n",
    "            \n",
    "            all_trials.append({\n",
    "                'trial': trial,\n",
    "                'response': response,\n",
    "                'metrics': metrics\n",
    "            })\n",
    "            \n",
    "            print(f\"Latency: {latency:.2f}s\")\n",
    "        \n",
    "        # Aggregate\n",
    "        aggregated = self.aggregate_metrics(all_trials)\n",
    "        \n",
    "        return {\n",
    "            'model': model.display_name,\n",
    "            'trials': all_trials,\n",
    "            'aggregated': aggregated\n",
    "        }\n",
    "    \n",
    "    def aggregate_metrics(self, trials: List[Dict]) -> Dict:\n",
    "        \"\"\"Aggregate metrics across trials\"\"\"\n",
    "        all_metrics = [t['metrics'] for t in trials]\n",
    "        aggregated = {}\n",
    "        \n",
    "        for key in all_metrics[0].keys():\n",
    "            values = [m[key] for m in all_metrics]\n",
    "            aggregated[key] = {\n",
    "                'mean': float(np.mean(values)),\n",
    "                'std': float(np.std(values)),\n",
    "                'min': float(np.min(values)),\n",
    "                'max': float(np.max(values))\n",
    "            }\n",
    "        \n",
    "        return aggregated\n",
    "\n",
    "print(\"? TrialEvaluator class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ?? Step 12: Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_comparison(results: List[Dict], save_path: str = './results'):\n",
    "    \"\"\"Create comprehensive comparison charts\"\"\"\n",
    "    \n",
    "    # Prepare data\n",
    "    metrics_to_plot = [\n",
    "        'latency', 'cosine_similarity', 'bertscore_f1', 'completeness',\n",
    "        'hallucination', 'irrelevance', 'meteor', 'bleu'\n",
    "    ]\n",
    "    \n",
    "    metric_labels = {\n",
    "        'latency': 'Latency (s)',\n",
    "        'cosine_similarity': 'Cosine Similarity',\n",
    "        'bertscore_f1': 'BERTScore F1',\n",
    "        'completeness': 'Completeness',\n",
    "        'hallucination': 'Hallucination',\n",
    "        'irrelevance': 'Irrelevance',\n",
    "        'meteor': 'METEOR',\n",
    "        'bleu': 'BLEU'\n",
    "    }\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8']\n",
    "    \n",
    "    for idx, metric in enumerate(metrics_to_plot):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        model_names = []\n",
    "        values = []\n",
    "        stds = []\n",
    "        \n",
    "        for result in results:\n",
    "            model_names.append(result['model'])\n",
    "            agg = result['aggregated'][metric]\n",
    "            values.append(agg['mean'])\n",
    "            stds.append(agg['std'])\n",
    "        \n",
    "        bars = ax.bar(model_names, values, yerr=stds, \n",
    "                     color=colors[:len(model_names)], alpha=0.8, capsize=5)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:.3f}',\n",
    "                   ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        ax.set_title(metric_labels[metric], fontsize=12, fontweight='bold')\n",
    "        ax.set_ylabel('Score', fontsize=10)\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.suptitle('Multi-Model RAG System Performance Comparison', \n",
    "                fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    save_file = os.path.join(save_path, 'comprehensive_comparison.png')\n",
    "    plt.savefig(save_file, dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\n? Chart saved: {save_file}\")\n",
    "    plt.show()\n",
    "\n",
    "def create_summary_table(results: List[Dict], save_path: str = './results'):\n",
    "    \"\"\"Create and save summary table\"\"\"\n",
    "    \n",
    "    data = []\n",
    "    for result in results:\n",
    "        agg = result['aggregated']\n",
    "        row = {\n",
    "            'Model': result['model'],\n",
    "            'Latency (s)': f\"{agg['latency']['mean']:.2f} ? {agg['latency']['std']:.2f}\",\n",
    "            'Cosine Sim': f\"{agg['cosine_similarity']['mean']:.3f}\",\n",
    "            'BERTScore F1': f\"{agg['bertscore_f1']['mean']:.3f}\",\n",
    "            'Completeness': f\"{agg['completeness']['mean']:.3f}\",\n",
    "            'Hallucination': f\"{agg['hallucination']['mean']:.3f}\",\n",
    "            'Irrelevance': f\"{agg['irrelevance']['mean']:.3f}\",\n",
    "            'METEOR': f\"{agg['meteor']['mean']:.3f}\",\n",
    "            'BLEU': f\"{agg['bleu']['mean']:.3f}\"\n",
    "        }\n",
    "        data.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"SUMMARY TABLE\")\n",
    "    print(\"=\"*100)\n",
    "    print(df.to_string(index=False))\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    # Save CSV\n",
    "    csv_file = os.path.join(save_path, 'summary_table.csv')\n",
    "    df.to_csv(csv_file, index=False)\n",
    "    print(f\"\\n? Table saved: {csv_file}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"? Visualization functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ?? Step 13: Define Models to Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models (you can modify these)\n",
    "MODEL_CONFIGS = [\n",
    "    {\n",
    "        'name': 'mistralai/Mistral-7B-Instruct-v0.2',\n",
    "        'display_name': 'Mistral-7B'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Qwen/Qwen2-7B-Instruct',\n",
    "        'display_name': 'Qwen3-7B'\n",
    "    },\n",
    "    {\n",
    "        'name': 'meta-llama/Llama-2-7b-chat-hf',\n",
    "        'display_name': 'Llama-2-7B'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Test questions\n",
    "TEST_CASES = [\n",
    "    {\n",
    "        'question': 'What is machine learning and what are its main types?',\n",
    "        'reference': 'Machine learning is a subset of artificial intelligence that enables systems to learn from data. The three main types are supervised learning (trained on labeled data), unsupervised learning (finds patterns in unlabeled data), and reinforcement learning (learns through interaction with environment).'\n",
    "    },\n",
    "    {\n",
    "        'question': 'What is Retrieval-Augmented Generation and what are its benefits?',\n",
    "        'reference': 'Retrieval-Augmented Generation (RAG) is an architecture that combines information retrieval with text generation to produce more accurate and factual responses. Key benefits include reduced hallucination, improved factual accuracy, and ability to use up-to-date information from documents.'\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"? Model configurations and test cases defined!\")\n",
    "print(f\"\\nModels to test: {len(MODEL_CONFIGS)}\")\n",
    "print(f\"Test questions: {len(TEST_CASES)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ?? Step 14: Run Complete Evaluation\n",
    "\n",
    "**Note**: This will take significant time and resources. For demo, you can:\n",
    "- Test with 1 model first\n",
    "- Reduce number of trials\n",
    "- Use smaller/lighter models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_complete_evaluation(rag_system, model_configs, test_cases, num_trials=3):\n",
    "    \"\"\"Run complete evaluation pipeline\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STARTING COMPLETE EVALUATION\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Models: {len(model_configs)}\")\n",
    "    print(f\"Test Cases: {len(test_cases)}\")\n",
    "    print(f\"Trials per model: {num_trials}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    trial_evaluator = TrialEvaluator(num_trials=num_trials)\n",
    "    all_results = []\n",
    "    \n",
    "    # For each test case\n",
    "    for case_idx, test_case in enumerate(test_cases, 1):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"TEST CASE {case_idx}/{len(test_cases)}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Question: {test_case['question']}\")\n",
    "        print(f\"Reference: {test_case['reference']}\")\n",
    "        \n",
    "        # For each model\n",
    "        for model_config in model_configs:\n",
    "            print(f\"\\n{'-'*80}\")\n",
    "            print(f\"Testing: {model_config['display_name']}\")\n",
    "            print(f\"{'-'*80}\")\n",
    "            \n",
    "            # Initialize model\n",
    "            model = LLMModel(\n",
    "                model_name=model_config['name'],\n",
    "                display_name=model_config['display_name']\n",
    "            )\n",
    "            \n",
    "            # Load model\n",
    "            if model.load(use_quantization=True):\n",
    "                # Run trials\n",
    "                result = trial_evaluator.run_trials(\n",
    "                    model=model,\n",
    "                    rag_system=rag_system,\n",
    "                    question=test_case['question'],\n",
    "                    reference=test_case['reference']\n",
    "                )\n",
    "                \n",
    "                all_results.append(result)\n",
    "                \n",
    "                # Print summary\n",
    "                agg = result['aggregated']\n",
    "                print(f\"\\n?? Summary for {model_config['display_name']}:\")\n",
    "                print(f\"  Latency: {agg['latency']['mean']:.2f}s (?{agg['latency']['std']:.2f})\")\n",
    "                print(f\"  Cosine Sim: {agg['cosine_similarity']['mean']:.3f}\")\n",
    "                print(f\"  BERTScore F1: {agg['bertscore_f1']['mean']:.3f}\")\n",
    "                print(f\"  Hallucination: {agg['hallucination']['mean']:.3f}\")\n",
    "                \n",
    "                # Unload model\n",
    "                model.unload()\n",
    "            else:\n",
    "                print(f\"?? Skipping {model_config['display_name']} due to load error\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Run evaluation (you can modify parameters)\n",
    "print(\"Starting evaluation...\")\n",
    "print(\"?? This will take significant time with multiple models!\")\n",
    "print(\"?? Tip: Start with 1 model for testing\")\n",
    "\n",
    "# Uncomment to run full evaluation:\n",
    "# results = run_complete_evaluation(\n",
    "#     rag_system=rag_system,\n",
    "#     model_configs=MODEL_CONFIGS,\n",
    "#     test_cases=TEST_CASES[:1],  # Use first test case only\n",
    "#     num_trials=3\n",
    "# )\n",
    "\n",
    "print(\"\\n? Evaluation function ready!\")\n",
    "print(\"\\n?? Uncomment the code above to run evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ?? Step 15: Generate Visualizations\n",
    "\n",
    "After running evaluation, generate charts and tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment after running evaluation:\n",
    "# if 'results' in locals() and results:\n",
    "#     print(\"\\n\" + \"=\"*80)\n",
    "#     print(\"GENERATING VISUALIZATIONS\")\n",
    "#     print(\"=\"*80)\n",
    "#     \n",
    "#     # Create charts\n",
    "#     plot_metrics_comparison(results)\n",
    "#     \n",
    "#     # Create summary table\n",
    "#     summary_df = create_summary_table(results)\n",
    "#     \n",
    "#     print(\"\\n\" + \"=\"*80)\n",
    "#     print(\"? EVALUATION COMPLETE!\")\n",
    "#     print(\"=\"*80)\n",
    "#     print(f\"Results saved in: ./results/\")\n",
    "# else:\n",
    "#     print(\"?? Run evaluation first to generate visualizations\")\n",
    "\n",
    "print(\"? Visualization code ready!\")\n",
    "print(\"\\n?? Uncomment after running evaluation to generate charts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ?? Quick Demo (Single Model)\n",
    "\n",
    "Test with a single lightweight model for quick results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_demo(rag_system, test_question, reference_answer):\n",
    "    \"\"\"Quick demo with lightweight model\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"QUICK DEMO\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Use a lightweight model for demo\n",
    "    demo_model = LLMModel('gpt2', 'GPT2-Demo')\n",
    "    \n",
    "    if demo_model.load(use_quantization=False):\n",
    "        # Get context\n",
    "        prompt, context = rag_system.query(test_question)\n",
    "        \n",
    "        print(f\"\\n?? Question: {test_question}\")\n",
    "        print(f\"\\n?? Retrieved Context ({len(context)} chars)\")\n",
    "        \n",
    "        # Generate\n",
    "        print(\"\\n?? Generating response...\")\n",
    "        response, latency = demo_model.generate(prompt, max_tokens=100)\n",
    "        \n",
    "        print(f\"\\n?? Response: {response}\")\n",
    "        print(f\"\\n?? Latency: {latency:.2f}s\")\n",
    "        \n",
    "        # Evaluate\n",
    "        evaluator = EvaluationMetrics()\n",
    "        metrics = evaluator.evaluate_all(\n",
    "            query=test_question,\n",
    "            response=response,\n",
    "            reference=reference_answer,\n",
    "            context=context,\n",
    "            latency=latency,\n",
    "            embedding_model=rag_system.vector_store.embedding_model\n",
    "        )\n",
    "        \n",
    "        print(\"\\n?? Metrics:\")\n",
    "        for metric, value in list(metrics.items())[:8]:\n",
    "            print(f\"  {metric:20s}: {value:.4f}\")\n",
    "        \n",
    "        demo_model.unload()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"? Demo complete!\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "# Run quick demo\n",
    "quick_demo(\n",
    "    rag_system,\n",
    "    \"What is machine learning?\",\n",
    "    \"Machine learning is a subset of AI that enables systems to learn from data.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ?? Summary\n",
    "\n",
    "### ? What We Built:\n",
    "\n",
    "1. **PDF Processing** - Load and chunk documents\n",
    "2. **Vector Database** - FAISS-based similarity search\n",
    "3. **RAG System** - Context-aware question answering\n",
    "4. **LLM Integration** - Support for Mistral, Qwen3, Llama\n",
    "5. **Comprehensive Metrics**:\n",
    "   - Latency\n",
    "   - Cosine Similarity\n",
    "   - BERTScore F1\n",
    "   - Completeness\n",
    "   - Hallucination Detection\n",
    "   - Irrelevance Detection\n",
    "   - METEOR, BLEU, ROUGE\n",
    "6. **Multi-Trial Testing** - Statistical aggregation\n",
    "7. **Visualizations** - Bar charts and tables\n",
    "\n",
    "### ?? How to Use:\n",
    "\n",
    "1. **Run all cells above**\n",
    "2. **Uncomment evaluation code** in Step 14\n",
    "3. **Choose models** to test (start with 1 for demo)\n",
    "4. **Run evaluation** (may take 10-30 min per model)\n",
    "5. **Generate visualizations** in Step 15\n",
    "6. **Download results** from `./results/` folder\n",
    "\n",
    "### ?? Tips:\n",
    "\n",
    "- **Memory**: Use GPU for faster inference\n",
    "- **Testing**: Start with 1 model and 1 trial\n",
    "- **PDFs**: Replace sample PDFs with your own\n",
    "- **Models**: Adjust MODEL_CONFIGS for different models\n",
    "- **Trials**: Increase num_trials for better statistics\n",
    "\n",
    "### ?? Resources:\n",
    "\n",
    "- **GitHub**: https://github.com/isratjahan829/LLM_Task\n",
    "- **Documentation**: See README.md\n",
    "\n",
    "---\n",
    "\n",
    "**?? Your RAG evaluation system is ready to use!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
