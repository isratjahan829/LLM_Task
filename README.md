# PDF RAG Evaluation Toolkit

This repository provides an offline evaluation harness for Retrieval-Augmented Generation
(RAG) systems that answer questions strictly using evidence retrieved from PDF documents.
It supports aggregating responses generated by multiple large language models (LLMs)
? for example Mistral, Qwen3, and LLaMA ? and computes a rich set of automatic metrics,
including latency, cosine similarity, F1, BERTScore, completeness, hallucination,
irrelevance, METEOR, BLEU, and average response length. The toolkit also produces
comparison plots that make it easy to inspect trial-to-trial differences across models.

## Project layout

```
src/
  evaluation/
    dataset.py        # Helpers for loading datasets and response logs
    metrics.py        # Metric implementations (cosine, F1, BERTScore, etc.)
    evaluator.py      # Orchestrates metric computation and summarises results
    plotting.py       # Generates comparison bar charts
scripts/
  run_offline_evaluation.py  # CLI entrypoint for batch evaluation
data/
  eval_dataset.jsonl         # Example reference dataset derived from PDFs
  responses/                 # Example model response logs for three trials
requirements.txt             # Python dependencies
```

## Getting started

1. **Install dependencies** (ideally inside a virtual environment):

   ```bash
   pip install -r requirements.txt
   ```

   > The metric suite uses PyTorch-backed models (`sentence-transformers` and `bert-score`).
   > Feel free to swap in lighter models through CLI flags if GPU memory is limited.

2. **Prepare your evaluation dataset** by exporting PDF-derived question/answer pairs
   to `JSONL`. Each line must include at least `id`, `question`, `reference_answer`, and
   optionally `reference_context` (the supporting PDF span). See `data/eval_dataset.jsonl`
   for a concrete template.

3. **Log model responses** from your RAG pipeline. For each model and trial, store the
   generated answer, optional latency, and retrieved context in a `JSONL` file. The
   expected schema aligns with the samples under `data/responses/`:

   ```json
   {"id": "sample-42", "trial": 1, "answer": "...", "latency": 1.37, "retrieved_context": "..."}
   ```

4. **Run the evaluation script** by pointing it to the dataset and your response logs.
   Multiple `--responses` arguments can be supplied ? one per model/trial combination.

   ```bash
   python scripts/run_offline_evaluation.py \
     --dataset data/eval_dataset.jsonl \
     --responses data/responses/mistral_trial1.jsonl:Mistral:1 \
     --responses data/responses/mistral_trial2.jsonl:Mistral:2 \
     --responses data/responses/mistral_trial3.jsonl:Mistral:3 \
     --responses data/responses/qwen_trial1.jsonl:Qwen3:1 \
     --responses data/responses/llama_trial1.jsonl:LLaMA:1
   ```

   Key outputs:

   - `reports/sample_metrics.csv` ? per-sample metric breakdown
   - `reports/summary_metrics.csv` & `.json` ? aggregated metrics per model/trial, including
     derived quality and latency scores
   - `plots/model_metric_comparison.png` ? bar chart faceted by metric for quick comparison

## Customising metrics and plots

- **Embedding & BERTScore models** can be overridden using `--embedding-model` and
  `--bert-score-model`. For CPU-only environments, consider lighter options such as
  `sentence-transformers/all-MiniLM-L6-v2` (default) and `distilbert-base-uncased`.

- **Plot metrics**: pass a filtered list via `--metrics` if you only need a subset, e.g.

  ```bash
  python scripts/run_offline_evaluation.py \
    --dataset data/eval_dataset.jsonl \
    --responses data/responses/mistral_trial1.jsonl:Mistral:1 \
    --metrics latency cosine_similarity f1 bert_score_f1
  ```

- **Additional signals**: Extend `src/evaluation/metrics.py` to include custom metrics
  such as ROUGE-L, contextual precision/recall, or judge-based hallucination scores.
  The `RAGEvaluator` automatically propagates new columns into the summaries and plots.

## Notes

- The provided heuristics for completeness, hallucination, and irrelevance rely on token
  overlap with the reference context/question. For higher-fidelity assessments consider
  integrating model-graded evaluations or knowledge-grounded QA benchmarks.

- Ensure that your PDF ingestion pipeline stores the retrieved context per question; the
  hallucination heuristic depends on this field to estimate unsupported content.